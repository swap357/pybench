name: Benchmark

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      build_from_source:
        description: 'Build Python from source'
        required: false
        default: 'false'

# Add permissions configuration
permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Cache Python versions
      uses: actions/cache@v4
      id: cache-python
      with:
        path: |
          ~/.pyenv/versions
          ~/.pyenv/shims
        key: ${{ runner.os }}-pyenv-versions-${{ hashFiles('pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pyenv-versions-

    - name: Install pyenv dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev \
          libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev

    - name: Install and configure pyenv
      run: |
        # Remove existing pyenv if any
        rm -rf ~/.pyenv
        
        # Fresh install of pyenv
        curl https://pyenv.run | bash
        
        # Configure pyenv
        echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc
        echo '[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc
        echo 'eval "$(pyenv init -)"' >> ~/.bashrc
        source ~/.bashrc

        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        pyenv --version

    # First install a default Python version for running setup
    - name: Install default Python
      run: |
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        pyenv install 3.12.7
        pyenv global 3.12.7

    # Install dependencies first
    - name: Install dependencies
      run: |
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        python -m pip install --upgrade pip
        pip install -e .

    # Now we can use the benchmark runner to get versions
    - name: Install Python versions
      run: |
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"

        # Get Python versions from the runner
        VERSIONS=$(python -c "from benchmark_runner import BenchmarkRunner; print(' '.join(BenchmarkRunner.PYTHON_VERSIONS.keys()))")
        
        if [ "${{ github.event.inputs.build_from_source }}" == "true" ]; then
          for version in $VERSIONS; do
            pyenv install -s $version --force
          done
        else
          for version in $VERSIONS; do
            if ! pyenv versions | grep -q $version; then
              pyenv install $version || pyenv install $version --patch < <(curl -sSL https://raw.githubusercontent.com/python/cpython/${version}/Misc/NEWS.d/${version}.rst)
            else
              echo "Python $version is already installed"
            fi
          done
        fi

        # Set global Python to baseline version
        BASELINE=$(python -c "from benchmark_runner import BenchmarkRunner; print(BenchmarkRunner.BASELINE_VERSION)")
        pyenv global $BASELINE
        python --version

    - name: Run benchmarks
      run: |
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init --path)"
        eval "$(pyenv init -)"
        python benchmark_runner.py --profile basic --report-format both
        ls -l  # Debug: list files to verify JSON was created

    - name: Install script dependencies
      run: |
        python -m pip install plotly kaleido beautifulsoup4

    - name: Generate Benchmark Report
      run: |
        ls -l  # Debug: verify files before running script
        python scripts/json_to_html.py --input-file benchmark_results.json --output-dir /tmp/benchmark_results

    - name: Publish Benchmark Results to GitHub Pages
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        # Save scripts and benchmark results
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        mkdir -p /tmp/benchmark_results
        cp -r scripts /tmp/benchmark_results/
        cp benchmark_results.json "/tmp/benchmark_results/${TIMESTAMP}.json"
        cp -r runs/* /tmp/benchmark_results/ || true
        
        # Debug: Show what we saved
        echo "Contents of /tmp/benchmark_results:"
        ls -la /tmp/benchmark_results/
        
        # Clean working directory
        git clean -fd
        
        # Fetch and checkout gh-pages branch
        echo "Fetching gh-pages branch..."
        git fetch origin gh-pages
        
        # Check if branch exists on remote and checkout
        if git ls-remote --heads origin gh-pages | grep gh-pages > /dev/null; then
          echo "Checking out existing gh-pages branch..."
          git checkout gh-pages
          git pull origin gh-pages
        else
          echo "Creating new gh-pages branch..."
          git checkout --orphan gh-pages
          git rm -rf .
          mkdir -p runs
          
          # Create initial index.html from saved template
          cp /tmp/benchmark_results/scripts/index_template.html index.html
        fi
        
        # Copy new benchmark results
        mkdir -p runs
        cp "/tmp/benchmark_results/${TIMESTAMP}.json" "runs/"
        cp -r /tmp/benchmark_results/*.html runs/ || true
        
        # Debug: Show contents before running update script
        echo "Contents before running update script:"
        ls -la
        ls -la runs/
        
        # Update index.html with new run
        python /tmp/benchmark_results/scripts/update_index.py --input-file "runs/${TIMESTAMP}.json" --index-file index.html
        
        # Clean up unnecessary files
        rm -rf __pycache__
        rm -rf *.pyc
        rm -rf python_benchmark_suite.egg-info
        rm -f benchmark_results.json
        
        # Stage all changes
        git add -A
        
        # Commit and push if there are changes
        if git status --porcelain | grep .; then
          git commit -m "Add benchmark results for run ${TIMESTAMP}"
          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin gh-pages
        else
          echo "No changes to commit"
        fi
